{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`written by Elijah DengDeng`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Python list to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化环境\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"lab3\") \n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = [[\"Chinese Beijing Chinese\", \"c\"],\\\n",
    "                [\"Chinese Chinese Nanjing\", \"c\"],\\\n",
    "                [\"Chinese Macao\", \"c\"],\\\n",
    "                [\"Australia Sydney Chinese\",\"o\"],\\\n",
    "               ]\n",
    "testData = [\"Chinese Chinese Chinese Australia Sydney\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(category='c', descript='Chinese Beijing Chinese'),\n",
       " Row(category='c', descript='Chinese Chinese Nanjing'),\n",
       " Row(category='c', descript='Chinese Macao'),\n",
       " Row(category='o', descript='Australia Sydney Chinese')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD = sc.parallelize(trainingData)\n",
    "testRDD = sc.parallelize(testData)\n",
    "trainRDD = trainRDD.map(lambda e: Row(descript=e[0], category=e[1]))\n",
    "testRDD = testRDD.map(lambda e: Row(descript=e))\n",
    "trainRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(descript='Chinese Chinese Chinese Australia Sydney')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|            descript|\n",
      "+--------+--------------------+\n",
      "|       c|Chinese Beijing C...|\n",
      "|       c|Chinese Chinese N...|\n",
      "|       c|       Chinese Macao|\n",
      "|       o|Australia Sydney ...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# covert to DataFrame\n",
    "trainDF = spark.createDataFrame(trainRDD)\n",
    "testDF = testRDD.toDF()\n",
    "# trainDF.createOrReplaceTempView(\"doc\") for sql \n",
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            descript|\n",
      "+--------------------+\n",
      "|Chinese Chinese C...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|       o|    1|\n",
      "|       c|    3|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normal operation for df using DSL syntax\n",
    "trainDF.groupby(\"category\").count().show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Tokenizer  把 descript spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|category|            descript|               words|\n",
      "+--------+--------------------+--------------------+\n",
      "|       c|Chinese Beijing C...|[chinese, beijing...|\n",
      "|       c|Chinese Chinese N...|[chinese, chinese...|\n",
      "|       c|       Chinese Macao|    [chinese, macao]|\n",
      "|       o|Australia Sydney ...|[australia, sydne...|\n",
      "+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "# defined a tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"descript\", outputCol=\"words\")\n",
    "# use the tokenizer\n",
    "tokenizedDF = tokenizer.transform(trainDF)\n",
    "\n",
    "testTokenizedDF = tokenizer.transform(testDF)\n",
    "# see the result\n",
    "tokenizedDF.select(\"category\", \"descript\", \"words\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            descript|               words|\n",
      "+--------------------+--------------------+\n",
      "|Chinese Chinese C...|[chinese, chinese...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testTokenizedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+------+\n",
      "|category|descript                |words                       |tokens|\n",
      "+--------+------------------------+----------------------------+------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |3     |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |3     |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |2     |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|3     |\n",
      "+--------+------------------------+----------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# user defined function\n",
    "countTokens = udf(lambda e: len(e))\n",
    "tokenizedDF = tokenizedDF.select(\"category\", \"descript\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\")))\n",
    "tokenizedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 CountVectorizer 把 words 转换成 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "|category|descript                |words                       |features                 |\n",
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |(6,[0,3],[2.0,1.0])      |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |(6,[0,2],[2.0,1.0])      |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |(6,[0,1],[1.0,1.0])      |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|(6,[0,4,5],[1.0,1.0,1.0])|\n",
      "+--------+------------------------+----------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "# define a CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")  # 可以带参数\n",
    "# fit with DataFrame to get a model\n",
    "cvModel = cv.fit(tokenizedDF)\n",
    "# use \n",
    "featuredDF = cvModel.transform(tokenizedDF)\n",
    "testFeaturedDF =cvModel.transform(testTokenizedDF)\n",
    "featuredDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            descript|               words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Chinese Chinese C...|[chinese, chinese...|(6,[0,4,5],[3.0,1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testFeaturedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[chinese, australia, sydney,macao, nanjing, beijing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `(6,[0,5],[2.0,1.0])` 是 `sparse Vector` 的形式  \n",
    "* 6 = `vocabulary size`\n",
    "* `[0,5]` 是 index\n",
    "* `[2.0,1.0]` 是 value\n",
    "* 等价于 dense Vector `[2.0, 0.0, 0.0, 0.0, 0.0, 1.0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 StringIndexer 把 label 转换成 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------------------------+------+-------------------------+-----+\n",
      "|category|descript                |words                       |tokens|features                 |label|\n",
      "+--------+------------------------+----------------------------+------+-------------------------+-----+\n",
      "|c       |Chinese Beijing Chinese |[chinese, beijing, chinese] |3     |(6,[0,1],[2.0,1.0])      |0.0  |\n",
      "|c       |Chinese Chinese Nanjing |[chinese, chinese, nanjing] |3     |(6,[0,2],[2.0,1.0])      |0.0  |\n",
      "|c       |Chinese Macao           |[chinese, macao]            |2     |(6,[0,5],[1.0,1.0])      |0.0  |\n",
      "|o       |Australia Sydney Chinese|[australia, sydney, chinese]|3     |(6,[0,3,4],[1.0,1.0,1.0])|1.0  |\n",
      "+--------+------------------------+----------------------------+------+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "indexedDF = indexer.fit(featuredDF).transform(featuredDF)\n",
    "indexedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+\n",
      "|features                 |label|\n",
      "+-------------------------+-----+\n",
      "|(6,[0,1],[2.0,1.0])      |0.0  |\n",
      "|(6,[0,2],[2.0,1.0])      |0.0  |\n",
      "|(6,[0,5],[1.0,1.0])      |0.0  |\n",
      "|(6,[0,3,4],[1.0,1.0,1.0])|1.0  |\n",
      "+-------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDF = indexedDF.select(\"features\", \"label\")\n",
    "finalDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Navie Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意使用 ml 的 NaiveBayes 而不是 mllib 的！！！！！！！\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='label', predictionCol='nb_prediction', smoothing=1.0, modelType='multinomial',)\n",
    "\n",
    "nb_model = nb.fit(finalDF)\n",
    "nb_model.transform(testFeaturedDF).head().nb_prediction  # 0.0 c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|category|            descript|\n",
      "+--------+--------------------+\n",
      "|       c|Chinese Beijing C...|\n",
      "|       c|Chinese Chinese N...|\n",
      "|       c|       Chinese Macao|\n",
      "|       o|Australia Sydney ...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            descript|\n",
      "+--------------------+\n",
      "|Chinese Chinese C...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|            descript|               words|            features|       rawPrediction|         probability|nb_prediction|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "|Chinese Chinese C...|[chinese, chinese...|(6,[0,2,3],[3.0,1...|[-8.2254733485002...|[0.59713120479585...|          0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "nb_pipeline = Pipeline(stages=[tokenizer, cv, indexer, nb])\n",
    "pipeModel = nb_pipeline.fit(trainDF)\n",
    "predictionDF = pipeModel.transform(testDF)\n",
    "predictionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
